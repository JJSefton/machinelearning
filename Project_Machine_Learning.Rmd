---
title: "Project Machine Learning"
output: html_document
---


#Dataset and background#
The dataset is an outcome of the HAR - Human Activity Recognition project. It has 159 explanatory variables and 1 outcome variable: the "classe" variable which codes in 5 factor levels the following activities:
A - Sitting
B - Sitting down
C - Standing
D - Standing up
E - Walking

#Goal and strategy#
Our goal is to:
1. build a machine learning model 
    . based on a subset of the 159 variables that explains the most variability of the outcome "classe".
    . based on a subset of exploitable observations out of the 19622 observations in the training data set.
2. cross validate the model on the testing data set
    . analysing OOB error measurement in the training set
    . splitting the training set into 60% for building the model and 40% for testing.
    . doing a confusion matrix
    . doing a final validation of the model on the 20 records (testing data) provided.


##Load data##
We're loading two csv files: one with training data and the other with testing data.
```{r LOAD}
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile="~/Desktop/training.csv")
training<-read.csv("~/Desktop/training.csv",na.strings=c("NA","#DIV/0!",""))
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile="~/Desktop/testing.csv")
testing<-read.csv("~/Desktop/testing.csv",na.strings=c("NA","#DIV/0!",""))
```

##Tidyingdata set##
```{r TIDY}
library(caret)
table(colSums(is.na(training)/dim(training[2])==TRUE)<.5)
train1<-training[,colSums(is.na(training)/dim(training[2])==TRUE)<.5]
train<-train1[,names(train1[,grepl("accel",names(train1))])]
train$classe<-train1$classe
```
The training dataset needs to be tidyed up. We decide to remove the variables where we're missing more than 50% of the observations. A new training data set is defined (train1) which contains still `r dim(train1)[1]` observations but only `r dim(train1)[2]` variables. Further we remove all variables non related to the accelerometer to reduce the exercise to motion detecting variables since the outcome is related to boddy motion. Our training dataset (train) has now `r dim(train)[2]` variables and the same number of observations.

###Model building
We take 60% of the train data set for training, leaving 40% for testing and decide to apply a random forest classification method to understand the source of variability in the outcome variable "classe". 
```{r MODEL}
library(randomForest)
set.seed(12345)
inTrain<-createDataPartition(y=train$classe,
                            p=0.6,list=FALSE)
trainset<-train[inTrain,]
testset<-train[-inTrain,]
fitModel <- randomForest(classe ~. , data=trainset, method="class")
```

##Model analysis##
The out of bag error (OOB) is 6%. This measures the internal error rate comparing 2/3 of the training set to the remaining 1/3. The classification error in the confusion matrix shows very low error rates < 0.1. 
```{r ANAL}
fitModel
prediction<-predict(fitModel,testset)
CM<-confusionMatrix(testset$classe,prediction)
CM
```
We use the test data set on the model now. This gives us a prediction based on the test data. We then use this prediction against the outcome variable "classe" in the test data set to check the robustness of the prediction. Ideally all observations should be in the diagonal of the confusionMatrix since this would mean that the prediction agrees with the test set.

We find our model has an overall accuracy of `r round(CM$overall[[1]],2)`. The observed accuracy compared to the expected accuracy is measured by the Kappa of the model. Here Kappa is `r round(CM$overall[[2]],2)`.

##Application to the final test##
We now take the same columns as we kept on the training dataset.
```{r TEST}
predictfinal<-predict(fitModel,testing)
predictfinal
```

